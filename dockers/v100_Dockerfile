# Dockerfile for v100 GPUs with CUDA 7.0 compatibility
# v100 GPUs support CUDA 7.0-11.8, but we'll use CUDA 11.8 for better vLLM compatibility

FROM nvidia/cuda:11.8.0-devel-ubuntu22.04

# Set environment variables
ENV DEBIAN_FRONTEND=noninteractive
ENV PYTHONUNBUFFERED=1
ENV CUDA_HOME=/usr/local/cuda
ENV PATH=${CUDA_HOME}/bin:${PATH}
ENV LD_LIBRARY_PATH=${CUDA_HOME}/lib64:${LD_LIBRARY_PATH}
ENV PYTHON_VERSION=310

# Install system dependencies
RUN apt-get update && apt-get install -y \
    python3.10 \
    python3.10-dev \
    python3-pip \
    git \
    wget \
    curl \
    build-essential \
    cmake \
    pkg-config \
    tmux \
    && rm -rf /var/lib/apt/lists/*

# Create symbolic link for python
RUN ln -sf /usr/bin/python3.10 /usr/bin/python

# Upgrade pip
RUN python -m pip install --upgrade pip

# vllm will install torch, so we don't need to install torch explicitly
RUN pip install https://github.com/vllm-project/vllm/releases/download/v0.4.0/vllm-0.4.0+cu118-cp310-cp310-manylinux1_x86_64.whl --extra-index-url https://download.pytorch.org/whl/cu118

# Python dependencies
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# Default requirements
RUN pip install --no-cache-dir \
    numpy==1.24.0 \
    accelerate==1.8.1

# Set environment variables for v100 optimization
ENV PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:128

# Set working directory
WORKDIR /app

ENTRYPOINT []

CMD ["bash"]
